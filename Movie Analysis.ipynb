{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy and pandas packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Reading and Inspection\n",
    "\n",
    "-  ### Subtask 1.1: Import and read\n",
    "\n",
    "Import and read the movie database. Store it in a variable called `movies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'movies_metadata.csv' does not exist: b'movies_metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-65a0b75ac742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read the csv file using 'read_csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmovies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'movies_metadata.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmovies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roshan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roshan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roshan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roshan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roshan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'movies_metadata.csv' does not exist: b'movies_metadata.csv'"
     ]
    }
   ],
   "source": [
    "# Read the csv file using 'read_csv'\n",
    "\n",
    "movies = pd.read_csv('movies_metadata.csv')\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 1.2: Inspect the dataframe\n",
    "\n",
    "Inspect the dataframe's columns, shapes, variable types etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows and columns in the dataframe\n",
    "\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column-wise info of the dataframe\n",
    "\n",
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the dataframe using 'describe()'\n",
    "\n",
    "movies.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Cleaning the Data\n",
    "\n",
    "-  ### Subtask 2.1: Inspect Null values\n",
    "\n",
    "Find out the number of Null values in all the columns and rows. Also, find the percentage of Null values in each column. Round-off the percentages upto two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column-wise Null count using 'is.null()' alongwith the 'sum()' function\n",
    "\n",
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the row-wise Null count the same way. This time just specify the axis as 1\n",
    "\n",
    "movies.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentages by dividing the sum obtained previously by the total length, multiplying it by 100 and rounding it off to\n",
    "# two decimal places\n",
    "\n",
    "round(100*(movies.isnull().sum()/len(movies.index)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.2: Drop unecessary columns\n",
    "\n",
    "For this assignment, you will mostly be analyzing the movies with respect to the ratings, gross collection, popularity of movies, etc. So many of the columns in this dataframe are not required. So it is advised to drop the following columns.\n",
    "-  color\n",
    "-  director_facebook_likes\n",
    "-  actor_1_facebook_likes\n",
    "-  actor_2_facebook_likes\n",
    "-  actor_3_facebook_likes\n",
    "-  actor_2_name\n",
    "-  cast_total_facebook_likes\n",
    "-  actor_3_name\n",
    "-  duration\n",
    "-  facenumber_in_poster\n",
    "-  content_rating\n",
    "-  country\n",
    "-  movie_imdb_link\n",
    "-  aspect_ratio\n",
    "-  plot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'drop()' function to drop the unnecessary columns\n",
    "\n",
    "movies = movies.drop(['color', \n",
    "                      'director_facebook_likes', \n",
    "                      'actor_3_facebook_likes', \n",
    "                      'actor_1_facebook_likes', \n",
    "                      'cast_total_facebook_likes', \n",
    "                      'actor_2_facebook_likes', \n",
    "                      'duration', \n",
    "                      'facenumber_in_poster', \n",
    "                      'content_rating', \n",
    "                      'country', \n",
    "                      'movie_imdb_link', \n",
    "                      'aspect_ratio',\n",
    "                      'plot_keywords',\n",
    "                      'actor_2_name',\n",
    "                      'actor_3_name'], \n",
    "                       axis = 1)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataset. Notice only 13 columns are left.\n",
    "\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.3: Drop unecessary rows using columns with high NaN percentages\n",
    "\n",
    "On inspection you might notice that some columns have large percentage (greater than 5%) of Null values. Drop all the rows which have Null values for such columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the percentages of Null values again\n",
    "\n",
    "round(100*(movies.isnull().sum()/len(movies.index)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since 'gross' and 'budget' columns have large number of NaN values, drop all the rows with NaNs at this column using the\n",
    "# 'isnan' function of NumPy alongwith a negation '~'\n",
    "\n",
    "movies = movies[~np.isnan(movies['gross'])]\n",
    "movies = movies[~np.isnan(movies['budget'])]\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the percentages of NaN\n",
    "\n",
    "round(100*(movies.isnull().sum()/len(movies.index)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.4: Drop unecessary rows\n",
    "\n",
    "Some of the rows might have greater than five Null values. Such rows aren't of much use for the analysis and hence, should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rows for which the sum of Null is less than five are retained\n",
    "\n",
    "movies = movies[movies.isnull().sum(axis=1) <= 5]\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the percentages of NaN\n",
    "\n",
    "round(100*(movies.isnull().sum()/len(movies.index)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.5: Fill NaN values\n",
    "\n",
    "You might notice that the `language` column has some NaN values. Here, on inspection, you will see that it is safe to replace all the missing values with `'English'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the language column of the dataset\n",
    "\n",
    "movies['language'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NaN values with 'English' since most of the movies are in the English language\n",
    "\n",
    "movies.loc[pd.isnull(movies['language']), ['language']] = 'English'\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the percentages of NaNs\n",
    "\n",
    "round(100*(movies.isnull().sum()/len(movies.index)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.6: Check the number of retained rows\n",
    "\n",
    "You might notice that two of the columns viz. `num_critic_for_reviews` and `actor_1_name` have small percentages of NaN values left. You can let these columns as it is for now. Check the number and percentage of the rows retained after completing all the tasks above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of retained rows using 'len()'\n",
    "# Get the percentage of retained rows by dividing the current number of rows with initial number of rows\n",
    "\n",
    "print(len(movies.index))\n",
    "print(len(movies.index)/5042)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Data Analysis\n",
    "\n",
    "-  ### Subtask 3.1: Change the unit of columns\n",
    "\n",
    "Convert the unit of the `budget` and `gross` columns from `$` to `million $`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the 'gross' and 'budget' columns by 1000000 to convert '$' to 'million $'\n",
    "\n",
    "movies['gross'] = movies['gross']/1000000\n",
    "movies['budget'] = movies['budget']/1000000\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.2: Find the movies with highest profit\n",
    "\n",
    "    1. Create a new column called `profit` which contains the difference of the two columns: `gross` and `budget`.\n",
    "    2. Sort the dataframe using the `profit` column as reference.\n",
    "    3. Extract the top ten profiting movies in descending order and store them in a new dataframe - `top10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column named 'profit' by subtracting the 'budget' column from the 'gross' column\n",
    "\n",
    "movies['profit'] = movies['gross'] - movies['budget']\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe with the 'profit' column as reference using the 'sort_values' function. Make sure to set the argument\n",
    "# 'ascending' to 'False'\n",
    "\n",
    "movies = movies.sort_values(by = 'profit', ascending = False)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the top 10 profitable movies by using position based indexing. Specify the rows till 10 (0-9)\n",
    "\n",
    "top10 = movies.iloc[:10, ]\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.3: Drop duplicate values\n",
    "\n",
    "After you found out the top 10 profiting movies, you might have noticed a duplicate value. So, it seems like the dataframe has duplicate values as well. Drop the duplicate values from the dataframe and repeat `Subtask 3.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate values using 'drop_duplicates' function. All the columns for duplicate rows need to be dropped and thus,\n",
    "# the 'subset' argument is set to 'None'. The 'keep = first' indicates to retain the first row among the duplicate rows, and\n",
    "# 'inplace = True' performs the operation on the dataframe in place.\n",
    "\n",
    "movies.drop_duplicates(subset = None, keep = 'first', inplace = True)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 profitable movies by using position based indexing. Specify the rows till 10 (0-9)\n",
    "\n",
    "top10 = movies.iloc[:10, ]\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.4: Find IMDb Top 250\n",
    "\n",
    "    1. Create a new dataframe `IMDb_Top_250` and store the top 250 movies with the highest IMDb Rating (corresponding to the column: `imdb_score`). Also make sure that for all of these movies, the `num_voted_users` is greater than 25,000.\n",
    "Also add a `Rank` column containing the values 1 to 250 indicating the ranks of the corresponding films.\n",
    "    2. Extract all the movies in the `IMDb_Top_250` dataframe which are not in the English language and store them in a new dataframe named `Top_Foreign_Lang_Film`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the movies by IMDb score\n",
    "# Retain the movies with 'num_voted_users' greater than 25000\n",
    "# Use position based indexing to get the first 250 rows in the sorted dataframe\n",
    "# Create a new column rank which contains the rank from 1 to 250\n",
    "\n",
    "IMDb_Top_250 = movies.sort_values(by = 'imdb_score', ascending = False)\n",
    "IMDb_Top_250 = IMDb_Top_250.loc[IMDb_Top_250.num_voted_users > 25000]\n",
    "IMDb_Top_250 = IMDb_Top_250.iloc[:250, ]\n",
    "IMDb_Top_250['Rank'] = range(1,251)\n",
    "IMDb_Top_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the non-English language films using conditional label based indexing\n",
    "\n",
    "Top_Foreign_Lang_Film = IMDb_Top_250.loc[IMDb_Top_250['language'] != 'English']\n",
    "Top_Foreign_Lang_Film"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Subtask 3.5: Find the best directors\n",
    "\n",
    "    1. Group the dataframe using the `director_name` column.\n",
    "    2. Find out the top 10 directors for whom the mean of `imdb_score` is the highest and store them in a new dataframe `top10director`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table using 'director_name' as index, 'imdb_score' as values, and 'mean' as aggfunc\n",
    "# Sort the values by 'imdb_score'. Keep 'ascending' as 'False'\n",
    "# Extract the top 10 from the dataframe created\n",
    "\n",
    "# PS: If I had to find the worst 10 directors, I would have sorted the dataframe in an ascending order and again extrcated the first 10 rows\n",
    "\n",
    "director = movies.pivot_table(values = 'imdb_score', index = 'director_name', aggfunc = 'mean')\n",
    "director = director.sort_values(by = 'imdb_score', ascending = False)\n",
    "director = director.iloc[:10, ]\n",
    "director"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.6: Find popular genres\n",
    "\n",
    "You might have noticed the `genres` column in the dataframe with all the genres of the movies seperated by a pipe (`|`). Out of all the movie genres, the first two are most significant for any film.\n",
    "\n",
    "1. Extract the first two genres from the `genres` column and store them in two new columns: `genre_1` and `genre_2`. Some of the movies might have only one genre. In such cases, extract the single genre into both the columns, i.e. for such movies the `genre_2` should be the same as `genre_1`.\n",
    "2. Group the dataframe using `genre_1` as the primary column and `genre_2` as the secondary column.\n",
    "3. Find out the 5 most popular combo of genres by finding the mean of the gross values using the `gross` column and store them in a new dataframe named `PopGenre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the elements of the 'genre' column at the pipe characters ('|') using str.split()\n",
    "# Assign the first elements of the rows of 'genre' column to a new column named 'genre_1' using 'apply()' and 'lambda' functions\n",
    "# Some of the movies have only one genre. In such cases, assign the same genre to 'genre_2' as well\n",
    "\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "movies['genre_1'] = movies['genres'].apply(lambda x: x[0])\n",
    "movies['genre_2'] = movies['genres'].apply(lambda x : x[1] if len(x) > 1 else x[0])\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe using 'genre_1' as the primary column and 'genre_2' as secondary\n",
    "\n",
    "movies_by_segment = movies.groupby(['genre_1', 'genre_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe PopGenre which contains the 'mean' of the gross values of each combination of genres present\n",
    "# Sort this dataframe using the 'gross' column and use index-based positioning to find out the five most popular genre combos\n",
    "\n",
    "PopGenre = pd.DataFrame(movies_by_segment['gross'].mean()).sort_values(by = 'gross', ascending = False)\n",
    "PopGenre.iloc[:5, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.7: Find the critic-favorite and audience-favorite actors\n",
    "\n",
    "    1. Create three new dataframes namely, `Meryl_Streep`, `Leo_Caprio`, and `Brad_Pitt` which contain the movies in which the actors: 'Meryl Streep', 'Leonardo DiCaprio', and 'Brad Pitt' are the lead actors. Use only the `actor_1_name` column for extraction. Also, make sure that you use the names 'Meryl Streep', 'Leonardo DiCaprio', and 'Brad Pitt' for the said extraction.\n",
    "    2. Append the rows of all these dataframes and store them in a new dataframe named `Combined`.\n",
    "    3. Group the combined dataframe using the `actor_1_name` column.\n",
    "    4. Find the mean of the `num_critic_for_reviews` and `num_user_for_review` and identify the actors which have the highest mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe containing Meryl Streep movies in which she is the lead actor\n",
    "\n",
    "Meryl_Streep = movies.loc[movies.actor_1_name == 'Meryl Streep']\n",
    "Meryl_Streep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe containing Leonardo DiCaprio movies in which he is the lead actor\n",
    "\n",
    "Leo_Caprio = movies.loc[movies.actor_1_name == 'Leonardo DiCaprio']\n",
    "Leo_Caprio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe containing Brad Pitt movies in which he is the lead actor\n",
    "\n",
    "Brad_Pitt = movies.loc[movies.actor_1_name == 'Brad Pitt']\n",
    "Brad_Pitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three dataframes using 'pd.concat()'\n",
    "\n",
    "Combined = pd.concat([Meryl_Streep, Brad_Pitt, Leo_Caprio])\n",
    "Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'actor_1_name'\n",
    "\n",
    "Combined_by_segment = Combined.groupby('actor_1_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that we had some null values for the column 'num_critic_for_reviews'. Make sure that none of these null values are\n",
    "# present in the new dataframe - 'Combined' that we have created\n",
    "\n",
    "Combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean of 'num_user_for_reviews' for each of the actor. Notice, Leonardo's is the highest\n",
    "\n",
    "Combined_by_segment['num_user_for_reviews'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean of 'num_critic_for_reviews' for each of the actor. In this case as well, Leonardo is leading\n",
    "\n",
    "Combined_by_segment['num_critic_for_reviews'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
